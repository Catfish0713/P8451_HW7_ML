---
title: "P8451_HW7_ML"
author: "Ruixi Li"
date: "2024-02-28"
output: html_document
---

```{r libraries}
library(tidyverse)
library(caret)
library(pROC)
library(rpart.plot)
```

# Data preparation

```{r load data, message=FALSE, warning=FALSE}
# load the data, remove dupplicate and clean variable names
mi = read_csv("mi.data.csv") |> distinct() |> janitor::clean_names()|> select(-id)

#mi |> group_by(fc) |> count()

skimr::skim(mi)# all variables are numeric

# In machine learning, we don't need to recode the character variables into its 'label', instead we convert them as factor with set reference.
cate_var = c("sex","pulm_adema","fc","arr","diab","obesity","asthma","readmission")
# convert all categorical variables into factors
mi = mi |> mutate(across(all_of(cate_var),as.factor))

# set all categorical variables' reference group as '0'
mi = mi |> mutate(across(all_of(cate_var), ~relevel(.x, ref = "0")))

mi |> Amelia::missmap(main = "Missing values vs observed")

summary(mi$readmission)
# the categorical outcome is strongly imbalanced
```

# Data Partitioning and cross-validation

```{r partition_cv, message=FALSE, warning=FALSE}
set.seed(123)
training.data = mi$readmission |> createDataPartition(p=0.7, list=F)
train.data = mi[training.data, ]
test.data = mi[-training.data, ]

set.seed(123)
control = trainControl(method="cv", number=10, sampling="up")
# The professor said that we focus on proportion when deciding if we use over/uder sampling and focus on absolute sample size. I am not sure if there's golden rule for a "large/small" dataset, but for hw6, with 400+ "yes", I was advised to use upsampling. Maybe for this hw(150+ "yes"), I should also use upsampling. 
#Plus, the computational load is acceptable when using upsampling. I chose to use up sampling here.
```

# Model training

```{r model_training_rf, message=FALSE, warning=FALSE}
set.seed(123)

# Define the tuning grid
mtry = expand.grid(.mtry=seq(1, ncol(train.data), by = 1))

# Setting mtry to the total number of features minus one is a fairly aggressive strategy, allowing each tree in the forest to consider almost all available features when making splits. This can be beneficial in certain datasets where most features contribute to predicting the target variable but can also increase the risk of overfitting, especially if some features are very predictive and consistently selected in the trees, leading to more correlated trees.So I chose to tuning thr mtry through cross validation

# common ntree ranges from 100 to 1000, I tried 50, 100, 200, 300, ntree=100 produce the best performance, given other parameters constant.

rf.model = train(
  readmission ~., 
  data=train.data, 
  method="rf", 
  metric="Accuracy", 
  tuneGrid=mtry, 
  ntree=100, 
  trControl=control)

rf.model$bestTune
varImp(rf.model)
plot(varImp(rf.model))
confusionMatrix(rf.model)


```



```{r model_training_en, message=FALSE, warning=FALSE}
set.seed(123)
en.model = train(
                  readmission ~., 
                  data = train.data, 
                  method = "glmnet",
                  trControl =  control, 
                  preProc=c("center", "scale"),
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(-3, 3, length = 100)))
                )

#Print the values of alpha and lambda that gave best prediction
en.model$bestTune

# Model coefficients
coef(en.model$finalModel, en.model$bestTune$lambda)

confusionMatrix(en.model)

```

* Model Comparison:
While the random forest model shows a higher accuracy(0.9001) compared to the Elastic Net model(0.6154), using the accuracy solely can be misleading on imbalanced datasets, even after upsampling, because it doesn't account for the distribution of the different classes; it simply reflects the proportion of total correct predictions.


# Model Evaluation
Since the outcome is binary, I chose to use ROC curve to evaluate their performance. But the AUC value of the random forest is too low(0.55), which makes me doubt my choice of the model. So I did the prediction for both of the two models and compare them in ROC graphics. (Although the professor said in the last lecture that we always compare and choose optimal model before applying it to test data, it seems calculating ROC need the prediction values.) 

```{r model_evaluation_rf, message=FALSE, warning=FALSE}

#Create predictions in test set
pred.rf = rf.model |>
              predict(test.data)

eval.results=confusionMatrix(pred.rf, test.data$readmission, positive = "1")
print(eval.results)

#Create predictions as probabilities on test set 
pred.rf.prob = rf.model |> 
  predict(test.data, type = "prob")

```


```{r model_evaluation_en, message=FALSE, warning=FALSE}
#Create predictions in test set
pred.en = en.model |>
              predict(test.data)

eval.results = confusionMatrix(pred.en, test.data$readmission, positive = "1")
print(eval.results)

#Create predictions as probabilities on test set 
pred.en.prob = en.model |> 
  predict(test.data, type = "prob")

```


```{r ROC}
#Another potential evaluation: Area under the Receiver Operating Curve (AUROC)
analysis_rf = roc(response=test.data$readmission, predictor=pred.rf.prob[,2])
analysis_en = roc(response=test.data$readmission, predictor=pred.en.prob[,2])
plot(1-analysis_rf$specificities,analysis_rf$sensitivities,type="l",
ylab="Sensitivity",xlab="1-Specificity",col="red",lwd=2,
main = "ROC Curve for the two models")
lines(1-analysis_en$specificities, analysis_en$sensitivities, col="blue", lwd=2)
abline(a=0,b=1)
legend("bottom", legend=c("random forest", "elastic net"),
       col=c("red", "blue"), lwd=2)
auc_rf = auc(analysis_rf)
auc_rf
auc_en = auc(analysis_en)
auc_en
```

* the Elastic Net model had a higher balanced accuracy, higher AUC value(0.6063 vs. 0.5351) and the ROC curve of it also at the left top corner compared with the random forest model's ROC curve. That is, the Elastic Net model offers a more balanced approach to handling both classes, making it the better choice in scenarios where both sensitivity and specificity are important. If we really want to develop a methods that can be implemented in the clinical setting. We would better choose Elastic Net model.
